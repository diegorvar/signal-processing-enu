{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a89963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.img_tiles import GoogleTiles\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7946e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/Users/dlugardo/Desktop/data/ENU_v2' # path to the folder with the data \n",
    "# Loads the Metadata for all the station (latitude, longitude, and elevation)\n",
    "StationMetaData = df = pd.read_csv('/Users/dlugardo/Documents/GitHub/signal-processing-enu/GreenlandStations.csv')\n",
    "\n",
    "def get_data(location):\n",
    "    file_name = str(location) + '.ENU.txt'\n",
    "    path = os.path.join(data_folder, file_name)\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        data = np.loadtxt(path, skiprows=2)\n",
    "    else:\n",
    "        file_name = str(location) + '_ENU.txt'\n",
    "        path = os.path.join(data_folder, file_name)\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            data = np.loadtxt(path, skiprows=2)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Neither '{location}.ENU.txt' nor '{location}_ENU.txt' found in {data_folder}\")\n",
    "    return data\n",
    "\n",
    "def decimal_year_to_date(decimal_year):\n",
    "    \"\"\"\n",
    "    Converts a decimal year to a datetime.date object.\n",
    "    \"\"\"\n",
    "    year = int(decimal_year)\n",
    "    fractional_part = decimal_year - year\n",
    "\n",
    "    # Determine if it's a leap year for accurate day calculation\n",
    "    is_leap = (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
    "    days_in_year = 366 if is_leap else 365\n",
    "\n",
    "    # Calculate the number of days from the start of the year\n",
    "    days_offset = fractional_part * days_in_year\n",
    "\n",
    "    # Create a datetime object for January 1st of that year\n",
    "    start_of_year = datetime.date(year, 1, 1)\n",
    "\n",
    "    # Add the calculated offset in days\n",
    "    result_date = start_of_year + datetime.timedelta(days=days_offset)\n",
    "\n",
    "    return result_date\n",
    "\n",
    "def doy_to_angle(doy):\n",
    "    radians = doy * 2 * np.pi / 365\n",
    "    mean_angle = np.arctan2(np.mean(np.sin(radians)), np.mean(np.cos(radians)))\n",
    "    cos = np.cos(mean_angle)\n",
    "    return(cos)\n",
    "\n",
    "def circular_mean(degrees):\n",
    "    radians = np.deg2rad(np.array(degrees) * 360/365)\n",
    "    mean_angle = np.arctan2(np.mean(np.sin(radians)), np.mean(np.cos(radians)))\n",
    "    mean_angle = np.rad2deg(mean_angle) * 365/360\n",
    "    if mean_angle < 0:\n",
    "        mean_angle += 365\n",
    "    return mean_angle\n",
    "\n",
    "MAX_GAP_DAYS = 30 \n",
    "\n",
    "def find_longest_continuous_segment(dates, max_gap_days=30):\n",
    "    # Ensure dates are sorted\n",
    "    dates = np.sort(dates)\n",
    "    segments = []\n",
    "    current_segment = [dates[0]]\n",
    "\n",
    "    for i in range(1, len(dates)):\n",
    "        if (dates[i] - dates[i-1]).days <= max_gap_days:\n",
    "            current_segment.append(dates[i])\n",
    "        else:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = [dates[i]]\n",
    "    segments.append(current_segment)\n",
    "\n",
    "    # Return the longest continuous segment\n",
    "    return max(segments, key=len)\n",
    "\n",
    "def InterpRA(data, detrend, INTERP_LIMIT, wdays):   \n",
    "    time = data[:, 0]\n",
    "\n",
    "    if detrend:   \n",
    "        data[:, 1:4] = scipy.signal.detrend(data[:, 1:4], axis=0)\n",
    "        \n",
    "    # Convert decimal year to datetime\n",
    "    converted_dates = np.array([decimal_year_to_date(dy) for dy in time])\n",
    "    df = pd.DataFrame({'Date': pd.to_datetime(converted_dates)})\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    df['East'] = data[:, 1]\n",
    "    df['North'] = data[:, 2]\n",
    "    df['Vertical'] = data[:, 3]\n",
    "    df['Horizontal'] =  np.sqrt(data[:, 1] ** 2 + data[:, 2] **2)\n",
    "    df['3DDisp'] = np.cbrt(data[:, 1] ** 2 + data[:, 2] **2 + data[:, 3] **2)\n",
    "\n",
    "    df['year'] = df.index.year\n",
    "    df['doy'] = df.index.dayofyear\n",
    "\n",
    "    filled_list = []\n",
    "    for year, group in df.groupby('year'):\n",
    "        # Determine if leap year\n",
    "        is_leap = (pd.Timestamp(f'{year}-12-31').is_leap_year)\n",
    "        days_in_year = 366 if is_leap else 365\n",
    "\n",
    "        # Create full day-of-year range\n",
    "        full_range = pd.DataFrame({'doy': np.arange(1, days_in_year + 1)})\n",
    "        full_range['year'] = year\n",
    "\n",
    "        # Merge to include missing days as NaN\n",
    "        group = full_range.merge(group, on=['year', 'doy'], how='left')\n",
    "\n",
    "        # Remove duplicates by averaging\n",
    "        group = group.groupby(['year', 'doy'], as_index=False).mean()\n",
    "\n",
    "        # Restore datetime index\n",
    "        group['Date'] = pd.to_datetime(\n",
    "            group['year'].astype(str) + '-' + group['doy'].astype(str),\n",
    "            format='%Y-%j',\n",
    "            errors='coerce'\n",
    "        )\n",
    "        group = group.set_index('Date').sort_index()\n",
    "\n",
    "        # ðŸ”¹ Interpolate only small gaps\n",
    "        for col in ['East', 'North', 'Vertical', 'Horizontal', '3DDisp']:\n",
    "            group[col] = group[col].interpolate(\n",
    "                method=\"time\", limit=INTERP_LIMIT, limit_direction=\"both\"\n",
    "            )\n",
    "\n",
    "        filled_list.append(group)\n",
    "\n",
    "        \n",
    "    # Concatenate all years\n",
    "    df = pd.concat(filled_list).sort_index()\n",
    "    \n",
    "    # Apply rolling mean smoothing\n",
    "    df_rolling = df.rolling(\n",
    "        window=wdays, \n",
    "        center=True, \n",
    "        min_periods=wdays - int(wdays / 5)\n",
    "    ).mean()\n",
    "    df_rolling = df_rolling.dropna()\n",
    "\n",
    "    # Remove Feb 29 for consistency\n",
    "    df_rolling = df_rolling[df_rolling.index.strftime('%m-%d') != '02-29']\n",
    "    \n",
    "    # Reattach year, day of year, and month_day columns\n",
    "    df_rolling['year'] = df_rolling.index.year\n",
    "    df_rolling['month_day'] = df_rolling.index.strftime('%m-%d')\n",
    "    df_rolling['doy'] = df_rolling.index.dayofyear\n",
    "\n",
    "    leap_mask = df_rolling.index.is_leap_year & (df_rolling.index.month > 2)\n",
    "    df_rolling.loc[leap_mask, 'doy'] -= 1\n",
    "    \n",
    "    return df_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735faba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Skipping NGFJ: continuous segment too short.\n",
      "  â†’ Error processing station JWLF: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ File not found for station THU4. Skipping.\n",
      "  â†’ Error processing station JGBL: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station THU2: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station NNVN: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ File not found for station QENU. Skipping.\n",
      "  â†’ Error processing station YMER: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ File not found for station AAS2. Skipping.\n",
      "  â†’ Skipping EQNU: continuous segment too short.\n",
      "  â†’ Error processing station ISOR: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station TIMM: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station KMOR: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station BLAS: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station KLSQ: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station MARG: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n",
      "  â†’ Error processing station HRDG: Cannot save file into a non-existent directory: '/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Filter both data and error to this segment\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misin(converted_dates, continuous_segment)\n\u001b[1;32m     29\u001b[0m data \u001b[38;5;241m=\u001b[39m raw_data[mask, :]\n\u001b[1;32m     31\u001b[0m df_rolling \u001b[38;5;241m=\u001b[39m InterpRA(data, Detrend, INTERP_LIMIT, wdays)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py:1138\u001b[0m, in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;124;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m       [ True, False]])\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m element \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(element)\n\u001b[0;32m-> 1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _in1d(element, test_elements, assume_unique\u001b[38;5;241m=\u001b[39massume_unique,\n\u001b[1;32m   1139\u001b[0m              invert\u001b[38;5;241m=\u001b[39minvert, kind\u001b[38;5;241m=\u001b[39mkind)\u001b[38;5;241m.\u001b[39mreshape(element\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py:985\u001b[0m, in \u001b[0;36m_in1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    983\u001b[0m         mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(ar1), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ar2:\n\u001b[0;32m--> 985\u001b[0m             mask \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m (ar1 \u001b[38;5;241m==\u001b[39m a)\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# Otherwise use sorting\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stations_names_with_data, metadata_records = [], []\n",
    "\n",
    "MAX_GAP_DAYS = 60        # Maximum gap acceptable between data\n",
    "INTERP_LIMIT = 8         # Maximum of days to be interpolated\n",
    "wdays = 30               # Rolling Average window (number of days considered in the average)\n",
    "MIN_DAYS_PER_YEAR = 350  # Minimum days required to be considered as a full year. \n",
    "min_years_of_data = 3    # Minimum length of the continuous segment\n",
    "\n",
    "Save = True\n",
    "Detrend = False\n",
    "\n",
    "for station_name in StationMetaData.station:\n",
    "    try:\n",
    "        raw_data = get_data(station_name)\n",
    "        time = raw_data[:, 0]\n",
    "        converted_dates = np.array([decimal_year_to_date(dy) for dy in time])\n",
    "\n",
    "        # Find longest continuous segment of data\n",
    "        continuous_segment = find_longest_continuous_segment(\n",
    "            converted_dates, max_gap_days=MAX_GAP_DAYS\n",
    "        )\n",
    "\n",
    "        if len(continuous_segment) < 365 * min_years_of_data:  # optional: skip if segment is too short\n",
    "            print(f\"  â†’ Skipping {station_name}: continuous segment too short.\")\n",
    "            continue\n",
    "\n",
    "        # Filter both data and error to this segment\n",
    "        mask = np.isin(converted_dates, continuous_segment)\n",
    "        data = raw_data[mask, :]\n",
    "\n",
    "        df_rolling = InterpRA(data, Detrend, INTERP_LIMIT, wdays)\n",
    "\n",
    "        valid_years = df_rolling.groupby('year').filter(\n",
    "            lambda x: len(x) >= MIN_DAYS_PER_YEAR\n",
    "        )['year'].unique()\n",
    "        df_rolling = df_rolling[df_rolling['year'].isin(valid_years)]\n",
    "        \n",
    "        LTM = df_rolling.groupby('month_day')[['East', 'North', 'Vertical','Horizontal', '3DDisp']].mean()\n",
    "        stds = df_rolling.groupby('month_day')[['East', 'North', 'Vertical','Horizontal', '3DDisp']].std()\n",
    "        \n",
    "        stds.columns = ['East_sd', 'North_sd', 'Vertical_sd','Horizontal_sd', '3DDisp_sd']\n",
    "        LTM = LTM.join(stds)\n",
    "\n",
    "        LTM[['East','North','Vertical', 'Horizontal', '3DDisp' ]] = (\n",
    "            LTM[['East','North','Vertical', 'Horizontal', '3DDisp']].rolling(3, center=True, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        # Save LTM for this station\n",
    "        if Save is True:\n",
    "            LTM.to_csv(f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/NotDetrended/LTM/{station_name}_Daily{wdays}RollingLTM.csv', index=True)\n",
    "\n",
    "        stations_names_with_data.append(station_name)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  â†’ File not found for station {station_name}. Skipping.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"  â†’ Error processing station {station_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeec0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
