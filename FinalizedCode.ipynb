{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6105f08a",
   "metadata": {},
   "source": [
    "# Greenland Crust Deformation\n",
    "Code by Diego R. Varela Lugardo. Mentor: Surendra Adhikari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61680b6c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1eafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.img_tiles import GoogleTiles\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e99b86",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f89aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/Users/dlugardo/Desktop/data/ENU_v2' # path to the folder with the data \n",
    "# Loads the Metadata for all the station (latitude, longitude, and elevation)\n",
    "StationMetaData = df = pd.read_csv('/Users/dlugardo/Documents/GitHub/signal-processing-enu/GreenlandStations.csv')\n",
    "\n",
    "def get_data(location):\n",
    "    file_name = str(location) + '.ENU.txt'\n",
    "    path = os.path.join(data_folder, file_name)\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        data = np.loadtxt(path, skiprows=2)\n",
    "    else:\n",
    "        file_name = str(location) + '_ENU.txt'\n",
    "        path = os.path.join(data_folder, file_name)\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            data = np.loadtxt(path, skiprows=2)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Neither '{location}.ENU.txt' nor '{location}_ENU.txt' found in {data_folder}\")\n",
    "    return data\n",
    "\n",
    "def decimal_year_to_date(decimal_year):\n",
    "    \"\"\"\n",
    "    Converts a decimal year to a datetime.date object.\n",
    "    \"\"\"\n",
    "    year = int(decimal_year)\n",
    "    fractional_part = decimal_year - year\n",
    "\n",
    "    # Determine if it's a leap year for accurate day calculation\n",
    "    is_leap = (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
    "    days_in_year = 366 if is_leap else 365\n",
    "\n",
    "    # Calculate the number of days from the start of the year\n",
    "    days_offset = fractional_part * days_in_year\n",
    "\n",
    "    # Create a datetime object for January 1st of that year\n",
    "    start_of_year = datetime.date(year, 1, 1)\n",
    "\n",
    "    # Add the calculated offset in days\n",
    "    result_date = start_of_year + datetime.timedelta(days=days_offset)\n",
    "\n",
    "    return result_date\n",
    "\n",
    "def doy_to_angle(doy):\n",
    "    radians = doy * 2 * np.pi / 365\n",
    "    mean_angle = np.arctan2(np.mean(np.sin(radians)), np.mean(np.cos(radians)))\n",
    "    cos = np.cos(mean_angle)\n",
    "    return(cos)\n",
    "\n",
    "def circular_mean(degrees):\n",
    "    radians = np.deg2rad(np.array(degrees) * 360/365)\n",
    "    mean_angle = np.arctan2(np.mean(np.sin(radians)), np.mean(np.cos(radians)))\n",
    "    mean_angle = np.rad2deg(mean_angle) * 365/360\n",
    "    if mean_angle < 0:\n",
    "        mean_angle += 365\n",
    "    return mean_angle\n",
    "\n",
    "MAX_GAP_DAYS = 30 \n",
    "\n",
    "def find_longest_continuous_segment(dates, max_gap_days=30):\n",
    "    # Ensure dates are sorted\n",
    "    dates = np.sort(dates)\n",
    "    segments = []\n",
    "    current_segment = [dates[0]]\n",
    "\n",
    "    for i in range(1, len(dates)):\n",
    "        if (dates[i] - dates[i-1]).days <= max_gap_days:\n",
    "            current_segment.append(dates[i])\n",
    "        else:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = [dates[i]]\n",
    "    segments.append(current_segment)\n",
    "\n",
    "    # Return the longest continuous segment\n",
    "    return max(segments, key=len)\n",
    "\n",
    "def InterpRA(data, detrend, INTERP_LIMIT, wdays):   \n",
    "    time = data[:, 0]\n",
    "\n",
    "    if detrend:   \n",
    "        data[:, 1:4] = scipy.signal.detrend(data[:, 1:4], axis=0)\n",
    "        \n",
    "    # Convert decimal year to datetime\n",
    "    converted_dates = np.array([decimal_year_to_date(dy) for dy in time])\n",
    "    df = pd.DataFrame({'Date': pd.to_datetime(converted_dates)})\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    df['East'] = data[:, 1]\n",
    "    df['North'] = data[:, 2]\n",
    "    df['Vertical'] = data[:, 3]\n",
    "    df['Horizontal'] =  np.sqrt(data[:, 1] ** 2 + data[:, 2] **2)\n",
    "    df['3DDisp'] = np.cbrt(data[:, 1] ** 2 + data[:, 2] **2 + data[:, 3] **2)\n",
    "\n",
    "    df['year'] = df.index.year\n",
    "    df['doy'] = df.index.dayofyear\n",
    "\n",
    "    filled_list = []\n",
    "    for year, group in df.groupby('year'):\n",
    "        # Determine if leap year\n",
    "        is_leap = (pd.Timestamp(f'{year}-12-31').is_leap_year)\n",
    "        days_in_year = 366 if is_leap else 365\n",
    "\n",
    "        # Create full day-of-year range\n",
    "        full_range = pd.DataFrame({'doy': np.arange(1, days_in_year + 1)})\n",
    "        full_range['year'] = year\n",
    "\n",
    "        # Merge to include missing days as NaN\n",
    "        group = full_range.merge(group, on=['year', 'doy'], how='left')\n",
    "\n",
    "        # Remove duplicates by averaging\n",
    "        group = group.groupby(['year', 'doy'], as_index=False).mean()\n",
    "\n",
    "        # Restore datetime index\n",
    "        group['Date'] = pd.to_datetime(\n",
    "            group['year'].astype(str) + '-' + group['doy'].astype(str),\n",
    "            format='%Y-%j',\n",
    "            errors='coerce'\n",
    "        )\n",
    "        group = group.set_index('Date').sort_index()\n",
    "\n",
    "        # ðŸ”¹ Interpolate only small gaps\n",
    "        for col in ['East', 'North', 'Vertical', 'Horizontal', '3DDisp']:\n",
    "            group[col] = group[col].interpolate(\n",
    "                method=\"time\", limit=INTERP_LIMIT, limit_direction=\"both\"\n",
    "            )\n",
    "\n",
    "        filled_list.append(group)\n",
    "\n",
    "        \n",
    "    # Concatenate all years\n",
    "    df = pd.concat(filled_list).sort_index()\n",
    "    \n",
    "    # Apply rolling mean smoothing\n",
    "    df_rolling = df.rolling(\n",
    "        window=wdays, \n",
    "        center=True, \n",
    "        min_periods=wdays - int(wdays / 5)\n",
    "    ).mean()\n",
    "    df_rolling = df_rolling.dropna()\n",
    "\n",
    "    # Remove Feb 29 for consistency\n",
    "    df_rolling = df_rolling[df_rolling.index.strftime('%m-%d') != '02-29']\n",
    "    \n",
    "    # Reattach year, day of year, and month_day columns\n",
    "    df_rolling['year'] = df_rolling.index.year\n",
    "    df_rolling['month_day'] = df_rolling.index.strftime('%m-%d')\n",
    "    df_rolling['doy'] = df_rolling.index.dayofyear\n",
    "\n",
    "    leap_mask = df_rolling.index.is_leap_year & (df_rolling.index.month > 2)\n",
    "    df_rolling.loc[leap_mask, 'doy'] -= 1\n",
    "    \n",
    "    return df_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de420ab6",
   "metadata": {},
   "source": [
    "## Long Term Mean CSV creation\n",
    "The code initializes with five different variables that will be utilized in the helper functions. \n",
    "1. The code retrieves the raw data, extracting the time component  and transforming it from decimal to normal format. \n",
    "2. The code searches for the longest 'continuous' segment (maximum gap days are controlled by MAX_GAP_DAYS variable). \n",
    "3. New Data Array is produced by masking the raw data to get the longest 'continuous' segment.\n",
    "4. Data is passed through InterpRA function. The function does the following:\n",
    "    - If Detrend = True, it linearly detrends the data using scipy.signal\n",
    "    - Displacement and Error Data is extracted. Five timeseries are created for the displacement vectors: East, North, Vertical, Horizontal, 3DDisp (cubic root of the sum of the 1D displacement vectors).\n",
    "    - Linearly Interpolates missing values (variable here to set maximum of days)\n",
    "    - Rolling Average is produced\n",
    "    - Nan values are dropped \n",
    "    - February 29 is eliminated so that all years have 365 days\n",
    "    - New Dataset is returned\n",
    "5. If a specific day has less data points that accepted (min_years_per_doy), it gets masked.\n",
    "6. Long Term Mean (LTM) and Standard Deviation (StDevs) are computed for every day of the year for all five components.\n",
    "7. A rolling average (3 day window) is applied to the resultant LTM to smooth out. \n",
    "8. LTM and StDevs are exported as a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa18fcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Skipping NGFJ: continuous segment too short.\n",
      "  â†’ File not found for station THU4. Skipping.\n",
      "  â†’ File not found for station QENU. Skipping.\n",
      "  â†’ File not found for station AAS2. Skipping.\n",
      "  â†’ Skipping EQNU: continuous segment too short.\n",
      "  â†’ Skipping SCOB: continuous segment too short.\n",
      "  â†’ File not found for station SCO4. Skipping.\n",
      "  â†’ File not found for station STNO. Skipping.\n",
      "  â†’ File not found for station QAQ2. Skipping.\n",
      "  â†’ File not found for station KLQ3. Skipping.\n",
      "  â†’ Skipping UPAK: continuous segment too short.\n",
      "  â†’ Skipping AVAN: continuous segment too short.\n",
      "  â†’ File not found for station THU3. Skipping.\n",
      "  â†’ Skipping NUNA: continuous segment too short.\n",
      "  â†’ File not found for station KSUT. Skipping.\n"
     ]
    }
   ],
   "source": [
    "stations_names_with_data, metadata_records = [], []\n",
    "\n",
    "MAX_GAP_DAYS = 60        # Maximum gap acceptable between data\n",
    "INTERP_LIMIT = 8         # Maximum of days to be interpolated\n",
    "wdays = 30               # Rolling Average window (number of days considered in the average)\n",
    "MIN_DAYS_PER_YEAR = 350  # Minimum days required to be considered as a full year. \n",
    "min_years_of_data = 3    # Minimum length of the continuous segment\n",
    "\n",
    "Save = False\n",
    "\n",
    "for station_name in StationMetaData.station:\n",
    "    try:\n",
    "        raw_data = get_data(station_name)\n",
    "        time = raw_data[:, 0]\n",
    "        converted_dates = np.array([decimal_year_to_date(dy) for dy in time])\n",
    "\n",
    "        # Find longest continuous segment of data\n",
    "        continuous_segment = find_longest_continuous_segment(\n",
    "            converted_dates, max_gap_days=MAX_GAP_DAYS\n",
    "        )\n",
    "\n",
    "        if len(continuous_segment) < 365 * min_years_of_data:  # optional: skip if segment is too short\n",
    "            print(f\"  â†’ Skipping {station_name}: continuous segment too short.\")\n",
    "            continue\n",
    "\n",
    "        # Filter both data and error to this segment\n",
    "        mask = np.isin(converted_dates, continuous_segment)\n",
    "        data = raw_data[mask, :]\n",
    "\n",
    "        df_rolling = InterpRA(data, True, INTERP_LIMIT, wdays)\n",
    "\n",
    "        valid_years = df_rolling.groupby('year').filter(\n",
    "            lambda x: len(x) >= MIN_DAYS_PER_YEAR\n",
    "        )['year'].unique()\n",
    "        df_rolling = df_rolling[df_rolling['year'].isin(valid_years)]\n",
    "        \n",
    "        LTM = df_rolling.groupby('month_day')[['East', 'North', 'Vertical','Horizontal', '3DDisp']].mean()\n",
    "        stds = df_rolling.groupby('month_day')[['East', 'North', 'Vertical','Horizontal', '3DDisp']].std()\n",
    "        \n",
    "        stds.columns = ['East_sd', 'North_sd', 'Vertical_sd','Horizontal_sd', '3DDisp_sd']\n",
    "        LTM = LTM.join(stds)\n",
    "\n",
    "        LTM[['East','North','Vertical', 'Horizontal', '3DDisp' ]] = (\n",
    "            LTM[['East','North','Vertical', 'Horizontal', '3DDisp']].rolling(3, center=True, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        # Save LTM for this station\n",
    "        if Save is True:\n",
    "            LTM.to_csv(f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/LTM/files/{station_name}_Daily{wdays}RollingLTM.csv', index=True)\n",
    "\n",
    "        stations_names_with_data.append(station_name)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  â†’ File not found for station {station_name}. Skipping.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"  â†’ Error processing station {station_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d1057",
   "metadata": {},
   "source": [
    "## Plotting of the Data\n",
    "Note that the StDev is quite big as there is a lot of fluctuation in the mean of every year (Multidecadal systems). This StDev is not as useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78500e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "station = 'SRMP'\n",
    "climatology_daily = pd.read_csv('/Users/dlugardo/Documents/GitHub/signal-processing-enu/LTM/files/' + station + '_Daily30RollingLTM.csv') # ',{station_name}_Daily{wdays}RollingLTM.csv\n",
    "\n",
    "# Set up the figure and subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(22, 10), sharex=True)\n",
    "\n",
    "# Define colors\n",
    "colors = {'East': '#1f77b4', 'North': '#ff7f0e', 'Vertical': '#2ca02c'}\n",
    "\n",
    "# Plot each component\n",
    "components = ['East', 'North', 'Vertical']\n",
    "titles = ['East Detrended (mm)', 'North Detrended (mm)', 'Vertical Detrended (mm)']\n",
    "\n",
    "for i, component in enumerate(components):\n",
    "    ax = axs[i]\n",
    "    sd = climatology_daily[component + '_sd']\n",
    "    ax.plot(climatology_daily['month_day'], climatology_daily[component], label=f'{component.title()} Mean', color=colors[component])\n",
    "    ax.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "\n",
    "    # 1-sigma shading\n",
    "    ax.fill_between(climatology_daily['month_day'],\n",
    "                    climatology_daily[component] - sd,\n",
    "                    climatology_daily[component] + sd,\n",
    "                    color=colors[component], alpha=0.2, label='Â±1Ïƒ')\n",
    "\n",
    "    # 2-sigma only for 'up'\n",
    "    if component == 'Vertical':\n",
    "        ax.fill_between(climatology_daily['month_day'],\n",
    "                        climatology_daily[component] - 2 * sd,\n",
    "                        climatology_daily[component] + 2 * sd,\n",
    "                        color=colors[component], alpha=0.1, label='Â±2Ïƒ')\n",
    "\n",
    "    ax.set_title(titles[i], fontsize=13, weight='bold')\n",
    "    ax.grid(True, linestyle=':', linewidth=0.5)\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.set_ylabel('Displacement (mm)', fontsize=11)\n",
    "\n",
    "# X-axis settings\n",
    "#axs[-1].set_xlabel('Date (MM-DD)', fontsize=12)\n",
    "axs[-1].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "axs[-1].xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "# Rotate x labels\n",
    "plt.setp(axs[-1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'{station} Long Term Mean Displacement (30-Day Rolling Average)', fontsize=16, weight='bold', y=1.02)\n",
    "\n",
    "# Final layout tweaks\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812281da",
   "metadata": {},
   "source": [
    "## 'Compressed' Anomalies\n",
    "To observe the 'seasonal' behavior and take out multi-year oscillations, the data is compressed so that every year has an average of zero. \n",
    "Each year is averaged and the average is substracted to the same year, plotting the anomaly to respect that year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef19bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DispAll = ['Vertical', 'Horizontal', '3DDisp']\n",
    "\n",
    "for station_name in stations_names_with_data:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)  # 3 rows, 1 col\n",
    "\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    n_years = 0  # counter\n",
    "\n",
    "    for j, Disp in enumerate(DispAll):\n",
    "        ax = axes[j]  # select subplot\n",
    "        \n",
    "        raw_data = get_data(station_name)\n",
    "        INTERP_LIMIT = 3  \n",
    "        wdays = 30\n",
    "        rd_data = InterpRA(raw_data, True, INTERP_LIMIT, wdays )\n",
    "\n",
    "        years = sorted(np.unique(rd_data.year))\n",
    "        colormap = cm.get_cmap('viridis', len(years))\n",
    "\n",
    "        ax.set_ylabel(f'{Disp} Displacement (mm)', fontsize=12)\n",
    "        ax.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "        ax.set_xlim(0, 365)\n",
    "\n",
    "        days = np.arange(1, 366)\n",
    "        all_years_df = []\n",
    "\n",
    "        # Load LTM\n",
    "        LTM = pd.read_csv('/Users/dlugardo/Documents/GitHub/signal-processing-enu/LTM/files/SRMP_Daily30RollingLTM.csv')\n",
    "        LTM = LTM[Disp]\n",
    "        offset_LTM = np.array(LTM - np.nanmean(LTM))\n",
    "\n",
    "        for year in years:\n",
    "            data = rd_data[rd_data['year'] == year].copy()\n",
    "            if data[Disp].count() < 350:\n",
    "                continue\n",
    "            data[Disp] = data[Disp] - np.nanmean(data[Disp])\n",
    "            all_years_df.append(data)\n",
    "\n",
    "        if len(all_years_df) == 0:\n",
    "            continue  # skip if no valid years\n",
    "\n",
    "        all_years_df = pd.concat(all_years_df)\n",
    "        STD = np.array(all_years_df.groupby('doy')[Disp].std())\n",
    "\n",
    "        # update year count once (first subplot is enough)\n",
    "        if j == 0:\n",
    "            n_years = len(all_years_df['year'].unique())\n",
    "\n",
    "        # bands\n",
    "        upper1 = offset_LTM + STD\n",
    "        lower1 = offset_LTM - STD\n",
    "        upper2 = offset_LTM + STD * 2\n",
    "        lower2 = offset_LTM - STD * 2\n",
    "        upper15 = offset_LTM + STD * 1.8\n",
    "        lower15 = offset_LTM - STD * 1.8\n",
    "\n",
    "        # Plot mean and bands\n",
    "        ax.plot(days, offset_LTM, lw=2, c='red', label='Mean')\n",
    "        ax.fill_between(days, lower1, upper1, color='red', alpha=0.2, label='Â±1Ïƒ')\n",
    "        #ax.fill_between(days, lower2, upper2, color='red', alpha=0.1, label='Â±2Ïƒ')\n",
    "\n",
    "        # Collect handles for legend only once (first loop)\n",
    "        if j == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            legend_handles.extend(handles)\n",
    "            legend_labels.extend(labels)\n",
    "\n",
    "        for i, year in enumerate(sorted(all_years_df['year'].unique())):\n",
    "            yr = all_years_df[all_years_df['year'] == year].set_index(\"doy\")\n",
    "            yr_disp = yr[Disp]\n",
    "            idx = yr.index.values - 1  \n",
    "\n",
    "            yr_upper = upper15[idx]\n",
    "            yr_lower = lower15[idx]\n",
    "\n",
    "            mask_upper = yr_disp.values > yr_upper\n",
    "            mask_lower = yr_disp.values < yr_lower\n",
    "            outside_frac = (mask_upper.sum() + mask_lower.sum()) / len(yr_disp)\n",
    "\n",
    "            # if outside_frac >= 0.1:\n",
    "            #     ax.plot(yr.index, yr_disp.values, lw=1.5, color=colormap(i), label=str(year))\n",
    "            # else:\n",
    "            #     ax.plot(yr.index, yr_disp.values, lw=1.0, color=colormap(i), alpha=0.1)\n",
    "\n",
    "    # shared x-axis labels\n",
    "    axes[-1].set_xlabel(\"Day of Year\", fontsize=12)\n",
    "\n",
    "    # use f-string for dynamic year count\n",
    "    fig.suptitle(f'Detrended - Average Displacement at {station_name}\\n n = {n_years} years',\n",
    "                 fontsize=16, weight='bold')\n",
    "\n",
    "    # One legend for all, at the bottom\n",
    "    fig.legend(legend_handles, legend_labels,\n",
    "               loc=\"lower center\", ncol=len(legend_labels), frameon=True)\n",
    "\n",
    "    plt.tight_layout(rect=[0.0, 0.02, .98, 1.005])  # extra space for legend + title\n",
    "    filename = f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/LTM/AverageDisplacement/AvgDisplacement_{station_name}_Anomalies.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851400a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Calculate Amplitude, DoY of peak & minima, and correlation to the LTM. This uses the yearly anomaly data. It then plots it but does not store the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each displacement as its own figure.\n",
    "\n",
    "DispAll = ['Vertical','Horizontal', '3DDisp']\n",
    "\n",
    "for station_name in stations_names_with_data:\n",
    "    for Disp in DispAll:\n",
    "        # --- Load and preprocess data ---\n",
    "        raw_data = get_data(station_name)\n",
    "        INTERP_LIMIT = 3\n",
    "        wdays = 30\n",
    "        df_rolling = InterpRA(raw_data, True, INTERP_LIMIT, wdays)\n",
    "\n",
    "        years = sorted(np.unique(df_rolling.year))\n",
    "\n",
    "        # Load LTM\n",
    "        LTM = pd.read_csv('/Users/dlugardo/Documents/GitHub/signal-processing-enu/LTM/files/SRMP_Daily30RollingLTM.csv')\n",
    "        LTM_vals = LTM[Disp].values\n",
    "        offset_LTM = LTM_vals - np.nanmean(LTM_vals)\n",
    "\n",
    "        peak_doy, min_doy, amplitude, corr_ltm = [], [], [], []\n",
    "        years_used = []\n",
    "\n",
    "        for year in years:\n",
    "            data = df_rolling[df_rolling['year'] == year].copy()\n",
    "            if data.shape[0] < 350:\n",
    "                continue\n",
    "            \n",
    "            # Yearly anomaly\n",
    "            anomaly = data[Disp] - np.nanmean(data[Disp])\n",
    "            \n",
    "            # Peak/min DOY\n",
    "            max_idx = anomaly.idxmax()\n",
    "            min_idx = anomaly.idxmin()\n",
    "            peak_doy.append(data.loc[max_idx, 'doy'])\n",
    "            min_doy.append(data.loc[min_idx, 'doy'])\n",
    "\n",
    "            # Amplitude\n",
    "            amplitude.append(anomaly.max() - anomaly.min())\n",
    "\n",
    "            # Correlation with LTM\n",
    "            idx = data['doy'].values - 1  # align with LTM index\n",
    "            corr, _ = pearsonr(anomaly.values, offset_LTM[idx])\n",
    "            corr_ltm.append(corr)\n",
    "\n",
    "            years_used.append(str(year))\n",
    "\n",
    "        # Add circular mean / average column\n",
    "        peak_doy.append(circular_mean(peak_doy))\n",
    "        min_doy.append(circular_mean(min_doy))\n",
    "        amplitude.append(np.mean(amplitude))\n",
    "        corr_ltm.append(np.mean(corr_ltm))\n",
    "        years_used.append('Mean')\n",
    "\n",
    "        # --- Convert to arrays for heatmaps ---\n",
    "        arr_doy = np.vstack([peak_doy, min_doy])\n",
    "        arr_amp = np.array([amplitude])\n",
    "        arr_corr = np.array([corr_ltm])\n",
    "\n",
    "        # --- Create figure with height ratios {2,1,1} ---\n",
    "        fig, axes = plt.subplots(\n",
    "            3, 1, figsize=(9, 4), \n",
    "            constrained_layout=False, \n",
    "            gridspec_kw={'height_ratios': [1.8,1,1]}\n",
    "        )\n",
    "        plt.subplots_adjust(hspace=0)  # remove vertical gaps\n",
    "\n",
    "        # 1. Peak & Min DOY\n",
    "        im1 = axes[0].imshow(arr_doy, aspect='auto', cmap='plasma', vmin=0, vmax=365)\n",
    "        axes[0].set_yticks([0,1])\n",
    "        axes[0].set_yticklabels(['Peak DOY','Min DOY'])\n",
    "        axes[0].set_xticks(np.arange(len(years_used)))\n",
    "        axes[0].set_xticklabels(years_used, rotation=45)\n",
    "        fig.colorbar(im1, ax=axes[0], orientation='vertical', label='DOY', fraction=0.025, pad=0.02, shrink = 0.6)\n",
    "        for row in range(arr_doy.shape[0]):\n",
    "            axes[0].text(arr_doy.shape[1]-1, row, f\"{arr_doy[row,-1]:.1f}\",\n",
    "                        ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "        # 2. Amplitude\n",
    "        im2 = axes[1].imshow(arr_amp, aspect='auto', cmap='viridis')\n",
    "        axes[1].set_yticks([0])\n",
    "        axes[1].set_yticklabels(['Amplitude'])\n",
    "        axes[1].set_xticks(np.arange(len(years_used)))\n",
    "        axes[1].set_xticklabels(years_used, rotation=45)\n",
    "        fig.colorbar(im2, ax=axes[1], orientation='vertical', label='Amplitude', fraction=0.025, pad=0.02, shrink = 0.8)\n",
    "        axes[1].text(arr_amp.shape[1]-1, 0, f\"{arr_amp[0,-1]:.2f}\",\n",
    "                    ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "        # 3. Correlation with LTM\n",
    "        im3 = axes[2].imshow(arr_corr, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[2].set_yticks([0])\n",
    "        axes[2].set_yticklabels(['Corr LTM'])\n",
    "        axes[2].set_xticks(np.arange(len(years_used)))\n",
    "        axes[2].set_xticklabels(years_used, rotation=45)\n",
    "        fig.colorbar(im3, ax=axes[2], orientation='vertical', label='Correlation', fraction=0.025, pad=0.02, shrink = 0.8)\n",
    "        axes[2].text(arr_corr.shape[1]-1, 0, f\"{arr_corr[0,-1]:.2f}\",\n",
    "                    ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "        fig.suptitle(f\"{station_name} {Disp} Displacement Yearly Anomaly\", fontsize=16, y=0.93)\n",
    "        filename = f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/Visualization/Metrics/Metrics_{station_name}_{Disp}.png'\n",
    "        plt.savefig(filename, dpi=300)  \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b434a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All the displacements on a single figure.\n",
    "DispAll = ['Vertical','Horizontal','3DDisp']\n",
    "\n",
    "for station_name in stations_names_with_data:\n",
    "    # --- Load and preprocess data once ---\n",
    "    raw_data = get_data(station_name)\n",
    "    INTERP_LIMIT = 3\n",
    "    wdays = 30\n",
    "    df_rolling = InterpRA(raw_data, True, INTERP_LIMIT, wdays)\n",
    "\n",
    "    years = sorted(np.unique(df_rolling.year))\n",
    "\n",
    "    # Load LTM once\n",
    "    LTM = pd.read_csv('/Users/dlugardo/Documents/GitHub/signal-processing-enu/LTM/files/SRMP_Daily30RollingLTM.csv')\n",
    "    \n",
    "    # Containers for all displacements\n",
    "    all_peak_doy, all_min_doy, all_amplitude, all_corr_ltm = [], [], [], []\n",
    "\n",
    "    for Disp in DispAll:\n",
    "        LTM_vals = LTM[Disp].values\n",
    "        offset_LTM = LTM_vals - np.nanmean(LTM_vals)\n",
    "\n",
    "        peak_doy, min_doy, amplitude, corr_ltm = [], [], [], []\n",
    "        years_used = []\n",
    "\n",
    "        for year in years:\n",
    "            data = df_rolling[df_rolling['year'] == year].copy()\n",
    "            if data.shape[0] < 350:\n",
    "                continue\n",
    "            \n",
    "            anomaly = data[Disp] - np.nanmean(data[Disp])\n",
    "\n",
    "            # Peak/min DOY\n",
    "            max_idx = anomaly.idxmax()\n",
    "            min_idx = anomaly.idxmin()\n",
    "            peak_doy.append(data.loc[max_idx, 'doy'])\n",
    "            min_doy.append(data.loc[min_idx, 'doy'])\n",
    "\n",
    "            # Amplitude\n",
    "            amplitude.append(anomaly.max() - anomaly.min())\n",
    "\n",
    "            # Correlation with LTM\n",
    "            idx = data['doy'].values - 1\n",
    "            corr, _ = pearsonr(anomaly.values, offset_LTM[idx])\n",
    "            corr_ltm.append(corr)\n",
    "\n",
    "            years_used.append(str(year))\n",
    "\n",
    "        # Add mean row\n",
    "        peak_doy.append(circular_mean(peak_doy))\n",
    "        min_doy.append(circular_mean(min_doy))\n",
    "        amplitude.append(np.mean(amplitude))\n",
    "        corr_ltm.append(np.mean(corr_ltm))\n",
    "        \n",
    "        years_used.append('Mean')\n",
    "\n",
    "        # Collect for all displacements\n",
    "        all_peak_doy.append(peak_doy)\n",
    "        all_min_doy.append(min_doy)\n",
    "        all_amplitude.append(amplitude)\n",
    "        all_corr_ltm.append(corr_ltm)\n",
    "\n",
    "    # --- Convert to arrays for heatmaps ---\n",
    "    arr_doy = np.vstack([np.array(p) for pair in zip(all_peak_doy, all_min_doy) for p in pair])\n",
    "    # rows = [Peak_V, Min_V, Peak_H, Min_H, Peak_3D, Min_3D]\n",
    "    arr_amp = np.vstack(all_amplitude)   # shape (3, years+Mean)\n",
    "    arr_corr = np.vstack(all_corr_ltm)   # shape (3, years+Mean)\n",
    "\n",
    "    # --- Create figure ---\n",
    "    fig, axes = plt.subplots(\n",
    "        3, 1, figsize=(11, 6), \n",
    "        constrained_layout=False, \n",
    "        gridspec_kw={'height_ratios': [1.8,1,1]}\n",
    "    )\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "\n",
    "    # 1. Peak & Min DOY\n",
    "    im1 = axes[0].imshow(arr_doy, aspect='auto', cmap='plasma', vmin=0, vmax=365)\n",
    "    yticks = [f\"Peak {disp}\" for disp in DispAll] + [f\"Min {disp}\" for disp in DispAll]\n",
    "    axes[0].set_yticks(np.arange(arr_doy.shape[0]))\n",
    "    axes[0].set_yticklabels(yticks)\n",
    "    axes[0].set_ylabel(\"DOY\", fontsize=12, rotation=90, labelpad=10)\n",
    "    axes[0].set_xticks(np.arange(len(years_used)))\n",
    "    axes[0].set_xticklabels(years_used, rotation=45)\n",
    "    fig.colorbar(im1, ax=axes[0], orientation='vertical', label='DOY', fraction=0.025, pad=0.02, shrink=0.6)\n",
    "\n",
    "    for row in range(arr_doy.shape[0]):\n",
    "        axes[0].text(arr_doy.shape[1]-1, row, f\"{arr_doy[row,-1]:.1f}\",\n",
    "                     ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "    # 2. Amplitude\n",
    "    im2 = axes[1].imshow(arr_amp, aspect='auto', cmap='viridis')\n",
    "    axes[1].set_yticks(np.arange(len(DispAll)))\n",
    "    axes[1].set_yticklabels(DispAll)\n",
    "    axes[1].set_ylabel(\"Amplitude\", fontsize=12, rotation=90, labelpad=10)\n",
    "    axes[1].set_xticks(np.arange(len(years_used)))\n",
    "    axes[1].set_xticklabels(years_used, rotation=45)\n",
    "    fig.colorbar(im2, ax=axes[1], orientation='vertical', label='Amplitude', fraction=0.025, pad=0.02, shrink=0.8)\n",
    "\n",
    "    for row in range(arr_amp.shape[0]):\n",
    "        axes[1].text(arr_amp.shape[1]-1, row, f\"{arr_amp[row,-1]:.2f}\",\n",
    "                     ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "    # 3. Correlation with LTM\n",
    "    im3 = axes[2].imshow(arr_corr, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[2].set_yticks(np.arange(len(DispAll)))\n",
    "    axes[2].set_yticklabels(DispAll)\n",
    "    axes[2].set_ylabel(\"Corr LTM\", fontsize=12, rotation=90, labelpad=10)\n",
    "    axes[2].set_xticks(np.arange(len(years_used)))\n",
    "    axes[2].set_xticklabels(years_used, rotation=45)\n",
    "    fig.colorbar(im3, ax=axes[2], orientation='vertical', label='Correlation', fraction=0.025, pad=0.02, shrink=0.8)\n",
    "\n",
    "    for row in range(arr_corr.shape[0]):\n",
    "        axes[2].text(arr_corr.shape[1]-1, row, f\"{arr_corr[row,-1]:.2f}\",\n",
    "                     ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "    fig.suptitle(f\"{station_name} Displacement Yearly Anomalies\", fontsize=16, y=0.93)\n",
    "    filename = f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/Visualization/Metrics/Metrics_{station_name}.png'\n",
    "    plt.savefig(filename, dpi=300)  \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9d092",
   "metadata": {},
   "source": [
    "## Heatmap of Horizontal Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2863d",
   "metadata": {},
   "source": [
    "First block of the code creates a pandas dataframe with all the relevant metrics and the second one plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4651068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERP_LIMIT = 8\n",
    "wdays = 30\n",
    "\n",
    "DispAll = ['Horizontal', 'Vertical']\n",
    "records = []\n",
    "\n",
    "for station_name in stations_names_with_data:\n",
    "    raw_data = get_data(station_name)\n",
    "    df_rolling = InterpRA(raw_data, True, INTERP_LIMIT, wdays)\n",
    "\n",
    "    # station metadata\n",
    "    smdt = StationMetaData.loc[StationMetaData['station'] == station_name]\n",
    "    lon, lat = smdt.longitude, smdt.latitude\n",
    "\n",
    "    for year in sorted(np.unique(df_rolling.year)):\n",
    "        data = df_rolling[df_rolling['year'] == year].copy()\n",
    "        if data.shape[0] < 300:\n",
    "            # still record \"empty\" entry\n",
    "            for Disp in DispAll:\n",
    "                records.append({\n",
    "                    \"station\": station_name,\n",
    "                    \"lon\": lon,\n",
    "                    \"lat\": lat,\n",
    "                    \"year\": year,\n",
    "                    \"disp\": Disp,\n",
    "                    \"Peak_DoY\": np.nan,\n",
    "                    \"Min_Doy\": np.nan,\n",
    "                    \"Amplitude\": np.nan\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        for Disp in DispAll:\n",
    "            anomaly = data[Disp] - np.nanmean(data[Disp])\n",
    "            max_idx = anomaly.idxmax()\n",
    "            min_idx = anomaly.idxmin()\n",
    "\n",
    "            records.append({\n",
    "                \"station\": station_name,\n",
    "                \"lon\": lon,\n",
    "                \"lat\": lat,\n",
    "                \"year\": year,\n",
    "                \"disp\": Disp,\n",
    "                \"Peak_DoY\": data.loc[max_idx, \"doy\"],\n",
    "                \"Min_DoY\": data.loc[min_idx, \"doy\"],\n",
    "                \"Amplitude\": anomaly.max() - anomaly.min()\n",
    "            })\n",
    "\n",
    "# ---- Build DataFrame ----\n",
    "df_metrics = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (800, 1100) to (800, 1104) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[rawvideo @ 0x7fb247f12940] Stream #0: not enough frames to estimate rate; consider increasing probesize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error [Errno 8] nodename nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "DispAll = ['Horizontal', 'Vertical']#, '3DDisp']\n",
    "Metrics = [ 'Min_DoY']#, 'Amplitude'] 'Amplitude', 'Peak_DoY',\n",
    "\n",
    "tiler = GoogleTiles(style=\"satellite\")\n",
    "\n",
    "for Metric in Metrics:\n",
    "    for Disp in DispAll:\n",
    "        if Disp == 'Vertical' and Metric == 'Amplitude':\n",
    "            cmap = mpl.colormaps['jet'].resampled(20)\n",
    "            vmin = 0\n",
    "            vmax = 20\n",
    "            norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "        elif Disp == 'Horizontal' and Metric == 'Amplitude':\n",
    "            cmap = mpl.colormaps['jet'].resampled(20)\n",
    "            vmin = 0\n",
    "            vmax = 10\n",
    "            norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "        elif Metric == 'Peak_DoY' or 'Min_DoY':\n",
    "            cmap = mpl.colormaps['jet'].resampled(100)\n",
    "            vmin = 0\n",
    "            vmax = 365\n",
    "            norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "        filenames = []\n",
    "        for year in sorted(df_metrics[\"year\"].unique()):\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 11))\n",
    "            ax = plt.axes(projection=ccrs.Stereographic())\n",
    "            if Metric == 'Amplitude':\n",
    "                plt.title(f\"{Disp} Displacement (Peak-to-Peak Amplitude) ({year})\")\n",
    "\n",
    "            elif Metric == 'Peak_DoY':\n",
    "                plt.title(f\"{Disp} Day of Maximum Displacement ({year})\")\n",
    "            \n",
    "            elif Metric == 'Min_DoY':\n",
    "                plt.title(f\"{Disp} Day of Minimum Displacement ({year})\")\n",
    "\n",
    "            ax.set_extent([-55, -5, 55, 90])\n",
    "            ax.add_image(tiler,6)\n",
    "            ax.gridlines(draw_labels=True)\n",
    "            ax.stock_img()\n",
    "            ax.coastlines(resolution=\"10m\", alpha=0.3)\n",
    "\n",
    "            # Filter dataframe\n",
    "            df_sub = df_metrics[(df_metrics[\"disp\"] == Disp) & (df_metrics[\"year\"] == year)]\n",
    "\n",
    "            for _, row in df_sub.iterrows():\n",
    "                if np.isnan(row[Metric]):\n",
    "                    # No data â†’ empty marker\n",
    "                    ax.scatter(\n",
    "                        row[\"lon\"], row[\"lat\"],\n",
    "                        facecolors=\"none\",\n",
    "                        edgecolors=\"k\",\n",
    "                        s=100,\n",
    "                        linewidth=1,\n",
    "                        transform=ccrs.PlateCarree()\n",
    "                    )\n",
    "                else:\n",
    "                    # Filled marker\n",
    "                    ax.scatter(\n",
    "                        row[\"lon\"], row[\"lat\"],\n",
    "                        c=[row[Metric]],\n",
    "                        cmap=cmap,\n",
    "                        norm=norm,\n",
    "                        s=150,\n",
    "                        edgecolor=\"k\",\n",
    "                        linewidth=0.5,\n",
    "                        alpha = 1,\n",
    "                        transform=ccrs.PlateCarree()\n",
    "                    )\n",
    "\n",
    "            # --- Colorbar ---\n",
    "            sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "            sm.set_array([])\n",
    "            cbar = plt.colorbar(sm, ax=ax, orientation=\"vertical\", shrink=0.6, pad=0.02)\n",
    "            if Metric == 'Amplitude':\n",
    "                cbar.set_label(\"Amplitude (mm)\")\n",
    "\n",
    "            else:\n",
    "               cbar.set_label(\"Day of Year\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            filename = f\"/Users/dlugardo/Documents/GitHub/signal-processing-enu/Visualization/PlotsforAnimations/{Disp}_{Metric}_{year}_Detrended_Yearly.png\"\n",
    "            plt.savefig(filename)\n",
    "            filenames.append(filename)\n",
    "            plt.close()\n",
    "        \n",
    "        # --- Build GIF --- \n",
    "        with imageio.get_writer(f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/Visualization/{Disp}_{Metric}_Detrended_Yearly.gif', mode='I', duration=300) as writer: \n",
    "            for filename in filenames: \n",
    "                image = imageio.imread(filename) \n",
    "                writer.append_data(image)\n",
    "        \n",
    "        # --- Build MP4 --- \n",
    "        with imageio.get_writer(f'/Users/dlugardo/Documents/GitHub/signal-processing-enu/Visualization/{Disp}_{Metric}_Detrended_Yearly.mp4', fps=2) as writer:\n",
    "            for filename in filenames: \n",
    "                image = imageio.imread(filename) \n",
    "                writer.append_data(image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
